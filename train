#!/usr/bin/python3

import argparse
import itertools

import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.autograd import Variable
from PIL import Image
import torch
import torch.nn.functional as F
import numpy as np

from models import Generator
from models import Generator1
from models import DecomNet
from models import RelightNet,RelightNet1
from models import Discriminator,Discriminator1
from models import DecomNet1
from utils import ReplayBuffer
from utils import LambdaLR
from utils import Logger
from utils import weights_init_normal
from datasets import ImageDataset
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '1'


# print(hasattr(torch.cuda, 'empty_cache'))
#
# torch.cuda.set_device(2)

parser = argparse.ArgumentParser()
parser.add_argument('--epoch', type=int, default=0, help='starting epoch')
parser.add_argument('--n_epochs', type=int, default=20, help='number of epochs of training')
parser.add_argument('--batchSize', type=int, default=4, help='size of the batches')
parser.add_argument('--dataroot', type=str, default='dataset/', help='root directory of the dataset')
parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate')
parser.add_argument('--decay_epoch', type=int, default=5, help='epoch to start linearly decaying the learning rate to 0')
parser.add_argument('--size', type=int, default=196, help='size of the data crop (squared assumed)')
parser.add_argument('--input_nc', type=int, default=3, help='number of channels of input data')
parser.add_argument('--output_nc', type=int, default=3, help='number of channels of output data')
parser.add_argument('--cuda', action='store_true', help='use GPU computation')
parser.add_argument('--n_cpu', type=int, default=0, help='number of cpu threads to use during batch generation')
parser.add_argument('--generator_A2B', type=str, default='output_checkpoint_12.5/wo_cycleI/netG_A2B.pth', help='A2B generator checkpoint file')
parser.add_argument('--generator_B2A', type=str, default='output_checkpoint_12.5/wo_cycleI/netG_B2A.pth', help='B2A generator checkpoint file')
parser.add_argument('--decom_net', type=str, default='output_checkpoint_12.5/wo_cycleI/Decom_net.pth', help='decom_net checkpoint file')
parser.add_argument('--denoise_net', type=str, default='output_checkpoint_12.5/wo_cycleI/Denoise_net.pth', help='denoise_net checkpoint file')
opt = parser.parse_args()
print(opt)


if torch.cuda.is_available() and not opt.cuda:
    print("WARNING: You have a CUDA device, so you should probably run with --cuda")



###### Definition of variables ######
# Networks define
netG_A2B = RelightNet()
netG_B2A = RelightNet()
Decom_net = DecomNet()
Denoise_net = RelightNet1()
netD_A = Discriminator(opt.input_nc)
netD_B = Discriminator(opt.output_nc)

#network to cude
if not opt.cuda:
    netG_A2B.cuda()
    netG_B2A.cuda()
    Decom_net.cuda()
    Denoise_net.cuda()
    netD_A.cuda()
    netD_B.cuda()
    # print("asd")

#generator A-B-A for L
netG_A2B.apply(weights_init_normal)
netG_B2A.apply(weights_init_normal)
# netG_A2B.load_state_dict(torch.load(opt.generator_A2B))
# netG_B2A.load_state_dict(torch.load(opt.generator_B2A))

#discrimanator for L
netD_A.apply(weights_init_normal)
netD_B.apply(weights_init_normal)

#decom netwwork for R and L
# Decom_net.apply(weights_init_normal)
Decom_net.load_state_dict(torch.load(opt.decom_net))

#denoise netwwork for R and L
# Denoise_net.apply(weights_init_normal)
Denoise_net.load_state_dict(torch.load(opt.denoise_net))

# Lossess
criterion_GAN = torch.nn.MSELoss()
l2_loss = torch.nn.MSELoss()
criterion_GAN = criterion_GAN.cuda()
criterion_cycle = torch.nn.L1Loss()
criterion_cycle = criterion_cycle.cuda()
criterion_identity = torch.nn.L1Loss()
criterion_identity = criterion_identity.cuda()

# Optimizers & LR schedulers
optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),
                                lr=opt.lr, betas=(0.9, 0.999))
# optimizer_Decom=torch.optim.Adam(Decom_net.parameters(),
#                                 lr=opt.lr, betas=(0.9, 0.999))
# optimizer_Denoise=torch.optim.Adam(Denoise_net.parameters(),
#                                 lr=opt.lr, betas=(0.9, 0.999))
optimizer_D_A = torch.optim.Adam(netD_A.parameters(),
                                lr=opt.lr, betas=(0.9, 0.999))
optimizer_D_B = torch.optim.Adam(netD_B.parameters(),
                                lr=opt.lr, betas=(0.9, 0.999))

lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)
lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)
lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)
# lr_scheduler_Decom = torch.optim.lr_scheduler.LambdaLR(optimizer_Decom, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)
# lr_scheduler_Denoise = torch.optim.lr_scheduler.LambdaLR(optimizer_Denoise, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)

# Inputs & targets memory allocation
Tensor = torch.cuda.FloatTensor if not opt.cuda else torch.Tensor
input_A = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)
input_B = Tensor(opt.batchSize, opt.output_nc, opt.size, opt.size)
target_real = Variable(Tensor(opt.batchSize).fill_(1.0), requires_grad=False)
target_fake = Variable(Tensor(opt.batchSize).fill_(0.0), requires_grad=False)

fake_A_buffer = ReplayBuffer()
fake_B_buffer = ReplayBuffer()

# Dataset loader
# transforms_ = [ transforms.Resize(int(opt.size*1.12), Image.BICUBIC),
#                 transforms.RandomCrop(opt.size),
#                 transforms.RandomHorizontalFlip(),
#                 transforms.ToTensor(),
#                 transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]
transforms_ = [ transforms.Resize(int(opt.size*1.12), Image.BICUBIC),
                transforms.RandomCrop(opt.size),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor()]
dataloader = DataLoader(ImageDataset(opt.dataroot, transforms_=transforms_, unaligned=True), 
                        batch_size=opt.batchSize, shuffle=True, num_workers=opt.n_cpu)

# Loss plot
# logger = Logger(opt.n_epochs, len(dataloader))
###################################

def gradient(input_tensor, direction):
    smooth_kernel_x = torch.FloatTensor([[0, 0], [-1, 1]]).view((1, 1, 2, 2)).cuda()
    smooth_kernel_y = torch.transpose(smooth_kernel_x, 2, 3)

    if direction == "x":
        kernel = smooth_kernel_x
    elif direction == "y":
        kernel = smooth_kernel_y
    grad_out = torch.abs(F.conv2d(input_tensor, kernel,
                                  stride=1, padding=1))
    return grad_out

def ave_gradient(input_tensor, direction):
    return F.avg_pool2d(gradient(input_tensor, direction),
                        kernel_size=3, stride=1, padding=1)

def smooth(input_I, input_R):
    input_R = 0.299 * input_R[:, 0, :, :] + 0.587 * input_R[:, 1, :, :] + 0.114 * input_R[:, 2, :, :]
    input_R = torch.unsqueeze(input_R, dim=1)
    # input_I = torch.mean(input_I,dim=1,keepdim=True)
    return torch.mean(gradient(input_I, "x") * torch.exp(-10 * ave_gradient(input_R, "x")) +
                      gradient(input_I, "y") * torch.exp(-10 * ave_gradient(input_R, "y")))

def exposure_control_loss(enhances, rsize=2, E=0.6):
    avg_intensity = F.avg_pool2d(enhances, rsize).mean(1)  # to gray: (R+G+B)/3
    exp_loss = (avg_intensity - E).abs().mean()
    return exp_loss

def noise_produce(image,mean=-100,val=0.01):
    size=image.shape
    gauss=np.random.normal(mean,val**0.5,size)
    # gauss=(gauss-np.min(gauss))/(np.max(gauss)-np.min(gauss))
    # print(np.min(gauss))
    # print(np.max(gauss))
    gauss=torch.tensor(gauss,dtype=torch.float)
    # gauss=torch.mean(gauss,dim=1,keepdim=True)
    return gauss

def concate(I):
    return torch.cat([I,I,I],dim=1)

def gradient_1order(x,h_x=None,w_x=None):
    if h_x is None and w_x is None:
        h_x = x.size()[2]
        w_x = x.size()[3]
    r = F.pad(x, (0, 1, 0, 0))[:, :, :, 1:]
    l = F.pad(x, (1, 0, 0, 0))[:, :, :, :w_x]
    t = F.pad(x, (0, 0, 1, 0))[:, :, :h_x, :]
    b = F.pad(x, (0, 0, 0, 1))[:, :, 1:, :]
    xgrad = torch.pow(torch.pow((r - l) * 0.5, 2) + torch.pow((t - b) * 0.5, 2)+0.0001, 0.5)
    return xgrad

a_rand_list=np.random.rand(300*1020)/100

###### Training ######
for epoch in range(opt.epoch, opt.n_epochs):
    if (epoch > 0):
        for i, batch in enumerate(dataloader):
            # Set model input
            # real_A = Variable(input_A.copy_(batch['A']))
            # real_B = Variable(input_B.copy_(batch['B']))
            real_A = batch['A'].cuda().clamp(0.0,1.0)
            real_B = batch['B'].cuda().clamp(0.0,1.0)
            # print(real_A.data)
            # print(real_A.shape)

            ###### Generators A2B and B2A ######
            optimizer_G.zero_grad()
            netG_A2B.zero_grad()
            netG_B2A.zero_grad()
            # optimizer_Decom.zero_grad()
            # Decom_net.zero_grad()
            # optimizer_Denoise.zero_grad()
            # Denoise_net.zero_grad()

            real_A_R,real_A_L=Decom_net(real_A) #R[:,3,:,:] L[:,1,:,:]
            # print(real_A_L.shape)
            real_B_R,real_B_L=Decom_net(real_B)
            real_A_L_3=concate(real_A_L)
            real_B_L_3=concate(real_B_L)
            # print(real_A_R.data)
            # Identity loss
            # G_A2B(B) should equal B if real B is fed
            real_B_L_same = netG_A2B(real_B_L,real_B_R)
            loss_identity_B = criterion_identity(real_B_L_same, real_B_L)*10.0
            # G_B2A(A) should equal A if real A is fed
            real_A_L_same=netG_B2A(real_A_L,real_A_R)
            loss_identity_A = criterion_identity(real_A_L_same, real_A_L)*10.0

            # GAN loss
            fake_B_L = netG_A2B(real_A_L,real_A_R)
            fake_B_R = Denoise_net(real_A_R)
            fake_B_L_3 = concate(fake_B_L)
            fake_B=fake_B_R*fake_B_L_3
            fake_B_R_o, fake_B_L_o = Decom_net(fake_B)
            fake_B_L_o_3 = concate(fake_B_L_o)
            pred_fake = netD_B(fake_B_L)
            loss_GAN_A2B = criterion_GAN(pred_fake, target_real)


            fake_A_L = netG_B2A(real_B_L,real_B_R)
            fake_A_L_3=concate(fake_A_L)
            noise=noise_produce(real_A,-0.25,a_rand_list[i*(epoch+1)]).cuda()*1
            # print(a_rand_list[i*(epoch+1)])
            # print(noise)
            fake_A=(real_B_R+noise).clamp(0.01,0.99)*fake_A_L_3
            fake_A_R_o, fake_A_L_o = Decom_net(fake_A)
            fake_A_L_o_3 = concate(fake_A_L_o)
            pred_fake = netD_A(fake_A_L)
            loss_GAN_B2A = criterion_GAN(pred_fake, target_real)

            # Cycle loss

            recovered_A_L = netG_B2A(fake_B_L_o,fake_B_R_o)
            recovered_A_L_3=concate(recovered_A_L)
            recovered_A=recovered_A_L_3*real_A_R
            loss_cycle_ABA = criterion_cycle(recovered_A_L, real_A_L)*10.0+criterion_cycle(recovered_A, real_A)*10.0

            # recovered_B = netG_A2B(fake_A)

            recovered_B_L = netG_A2B(fake_A_L_o,fake_A_R_o)
            recovered_B_R = Denoise_net(fake_A_R_o)
            recovered_B_L_3=concate(recovered_B_L)
            recovered_B = recovered_B_L_3*recovered_B_R
            loss_cycle_BAB = criterion_cycle(recovered_B_L, real_B_L)*10.0+criterion_cycle(recovered_B, real_B)*10.0

            loss_expo_nor = exposure_control_loss(fake_B_L,E=0.7)
            loss_expo_low = exposure_control_loss(fake_A_L, E=0.2)
            loss_expo=loss_expo_nor+0*loss_expo_low

            a=10000

            # Total loss
            loss_G =0*loss_identity_B+0*loss_identity_A+0.0001*loss_GAN_A2B +0.0001*loss_GAN_B2A + a*loss_cycle_ABA + a*loss_cycle_BAB + 0*loss_expo

            loss_denoise=100*criterion_cycle(recovered_B_R,real_B_R)+0*criterion_cycle(fake_B_R*real_A_L_3,real_A)

            # loss_denoise.backward()
            # optimizer_Denoise.step()

            loss_G.backward()
            optimizer_G.step()

            # loss_G.backward(retain_graph=True)

            self_recon_low_loss = criterion_cycle(real_A_R * real_A_L_3, real_A) + F.l1_loss(
                fake_A_R_o * fake_A_L_o_3, fake_A)
            self_recon_nor_loss = criterion_cycle(real_B_R * real_B_L_3, real_B) + F.l1_loss(
                fake_B_R_o * fake_B_L_o_3, fake_B)
            mul_recon_low_loss = criterion_cycle(fake_B_R_o * real_A_L_3, real_A) + F.l1_loss(
                real_B_R * fake_A_L_o_3, fake_A)
            mul_recon_nor_loss = criterion_cycle(fake_A_R_o * real_B_L_3, real_B) + F.l1_loss(
                real_A_R * fake_B_L_o_3, fake_B)
            equal_R_loss = 1*(1*F.l1_loss(real_A_R, fake_B_R_o) + 1*F.l1_loss(real_B_R,fake_A_R_o))
            equal_R_loss +=10*(F.l1_loss(gradient_1order(real_A_R),gradient_1order(fake_B_R_o))+F.l1_loss(gradient_1order(real_B_R),gradient_1order(fake_A_R_o)))

            #smooth L loss
            smooth_low_loss = smooth(real_A_L, real_A_R) + smooth(fake_A_L_o, fake_A_R_o)
            smooth_nor_loss = smooth(real_B_L, real_B_R) + smooth(fake_B_L_o, fake_B_R_o)
            smooth_loss=smooth_low_loss+smooth_nor_loss


            #L fidety loss with ini L
            ini_L_low,_ = torch.max(real_A,dim=1,keepdim=True)
            ini_L_nor,_ = torch.max(real_B, dim=1, keepdim=True)
            ini_L_low_fake, _ = torch.max(fake_A, dim=1, keepdim=True)
            ini_L_nor_fake, _ = torch.max(fake_B, dim=1, keepdim=True)
            fidety_L_low_loss = criterion_cycle(real_A_L,ini_L_low)+criterion_cycle(fake_A_L_o,ini_L_low_fake)
            fidety_L_nor_loss = criterion_cycle(real_B_L, ini_L_nor)+criterion_cycle(fake_B_L_o,ini_L_nor_fake)
            fidety_L_loss=fidety_L_low_loss+fidety_L_nor_loss

            decom_loss = self_recon_low_loss + self_recon_nor_loss + \
                         0.001 * mul_recon_low_loss + 0.001 * mul_recon_nor_loss + \
                         1 * smooth_loss + 0*fidety_L_loss + 0.01 * equal_R_loss

            decom_loss=50*decom_loss

            # decom_loss.backward()
            # optimizer_Decom.step()


            ###################################
            optimizer_D_A.zero_grad()
            netD_A.zero_grad()
            ###### Discriminator A ######
            # Real loss
            pred_real = netD_A(real_A_L.detach())
            loss_D_A_real = criterion_GAN(pred_real, target_real)

            # Fake loss
            # fake_A_L = fake_A_buffer.push_and_pop(fake_A_L)
            pred_fake = netD_A(fake_A_L.detach())
            loss_D_A_fake = criterion_GAN(pred_fake, target_fake)

            # Total loss
            loss_D_A = (loss_D_A_real + loss_D_A_fake)*0.5

            loss_D_A.backward()
            optimizer_D_A.step()

            ###### Discriminator B ######
            # Real loss
            optimizer_D_B.zero_grad()
            netD_B.zero_grad()

            pred_real = netD_B(real_B_L.detach())
            loss_D_B_real = criterion_GAN(pred_real, target_real)

            # Fake loss
            # fake_B_L = fake_B_buffer.push_and_pop(fake_B_L)
            pred_fake = netD_B(fake_B_L.detach())
            loss_D_B_fake = criterion_GAN(pred_fake, target_fake)

            # Total loss
            loss_D_B = (loss_D_B_real + loss_D_B_fake)*0.5

            loss_D_B.backward()
            optimizer_D_B.step()



            # Progress report (http://localhost:8097)
            if(i%10==0):
                print("epoch:" + str(epoch) + ",n_epoch:" + str(opt.n_epochs) + ",step:" + str(
                    i) + ",loss_G:" + str(loss_G.data))
                # logger.log({'loss_G': loss_G, 'decom_loss': (decom_loss),'cycle_loss':loss_cycle_ABA+loss_cycle_BAB,'loss_denoise':loss_denoise},
                #            images={'real_A': real_A, 'real_A_L': real_A_L,'fake_B_L': fake_B_L, 'fake_B_R_o': fake_B_R_o, 'fake_B': fake_B,
                #                    'real_B': real_B, 'real_B_L': real_B_L, 'fake_A_L': fake_A_L, 'recovered_B_R': recovered_B_R, 'fake_A': fake_A,'with_noise':(real_B_R+noise).clamp(0.01,0.99)})

        # Update learning rates
        lr_scheduler_G.step()
        lr_scheduler_D_A.step()
        lr_scheduler_D_B.step()
        # lr_scheduler_Decom.step()
        # lr_scheduler_Denoise.step()

        # Save models checkpoints
        torch.save(netG_A2B.state_dict(), 'output_checkpoint_12.5/100000/netG_A2B.pth')
        torch.save(netG_B2A.state_dict(), 'output_checkpoint_12.5/100000/netG_B2A.pth')
        torch.save(netD_A.state_dict(), 'output_checkpoint_12.5/100000/netD_A.pth')
        torch.save(netD_B.state_dict(), 'output_checkpoint_12.5/100000/netD_B.pth')
        # torch.save(Decom_net.state_dict(), 'output_checkpoint/Decom_net.pth')
        # torch.save(Denoise_net.state_dict(), 'output_checkpoint_12.5/wo_D/Denoise_net.pth')

